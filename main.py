import openai
import json
import os
import httpx
import asyncio
import pandas as pd
from datasets import load_dataset
import time
from pydantic import BaseModel
from typing import Literal

# --- –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è ---
# –£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ –≤–∞—à API-–∫–ª—é—á OpenAI —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω –∫–∞–∫ –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è –æ–∫—Ä—É–∂–µ–Ω–∏—è
os.environ["OPENAI_API_KEY"] = "sk-K0DkLtbB_....."

# URL of our local adapter from docker-compose
SEARXNG_ADAPTER_URL = "http://localhost:8000/search"

SYSTEM_PROMPT = """
# ROLE AND GOAL
You are a "Deep Research Agent", an advanced AI assistant. Your primary function is to conduct thorough, multi-step research to provide comprehensive, accurate, and well-supported answers to user queries. You are not a simple search engine; you are an analytical expert who synthesizes information from various sources.

# CORE PRINCIPLE: QUERY TRANSFORMATION & DECONSTRUCTION
This is your most critical instruction. You MUST NOT search for the user's exact query verbatim. Instead, you must first perform a "Query Analysis" step in your internal monologue (thought process).

1.  **Deconstruct:** Break down the user's query into its fundamental components: key entities, underlying questions, and the ultimate information goal.
2.  **Brainstorm & Rephrase:** Generate a set of 2-3 diverse and specific search queries. These queries should explore different angles, use synonyms, and target potentially relevant adjacent topics. This strategy is essential for discovering the most relevant sources, even if they don't match the user's original phrasing.
3.  **Hypothesize:** For each query, have a brief hypothesis about what kind of information it might yield.

# RESEARCH PROCESS
1.  **Iterative Search:** Execute your generated search queries one by one using the `search_the_web` tool.
2.  **Analyze & Adapt:** After each search (Observation), analyze the results.
    - Did you find relevant information?
    - Do the results from one search suggest a new, better query?
    - If a query fails, do not give up. Analyze why it failed and try a different, more refined query. This is a crucial part of the research process.
3.  **Synthesize:** Do not rely on a single source. Gather information from multiple relevant pages. Look for consensus and note any conflicting information. Your goal is to build a complete picture.

# FINAL ANSWER FORMULATION
Once you have gathered sufficient information from multiple search iterations and are confident in your findings, synthesize all the information into a final, coherent answer. The final answer should be short, clear, and directly answer the user's original query."""

SUMMARIZATION_PROMPT = """
You are an expert analyst. Your task is to extract key information from the provided text that directly answers the original user query.
Focus on facts, figures, main arguments, and conclusions. Ignore irrelevant details, boilerplate text, advertisements, and fluff.
The result should be a concise yet comprehensive summary of only the most critical data on the topic.

CRITICAL!
NEVER PROVIDE THE FINAL ANSWER YOURSELF! YOUR JOB IS TO EXTRACT THE MOST RELEVANT INFORMATION SO THAT A FINAL ANSWER CAN BE FORMED BASED ON IT.

Original user query: "{user_query}"

Text to analyze:
---
{content}
---

Extracted Information:
"""

class EvaluationResponse(BaseModel):
    evaluation: Literal["ok", "wrong"]

# --- –ù–æ–≤–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –æ—Ç–≤–µ—Ç–∞ ---
async def evaluate_answer(question: str, golden_answer: str, agent_answer: str) -> EvaluationResponse | str:
    """
    –°—Ä–∞–≤–Ω–∏–≤–∞–µ—Ç –æ—Ç–≤–µ—Ç –∞–≥–µ–Ω—Ç–∞ —Å "–∑–æ–ª–æ—Ç—ã–º" –æ—Ç–≤–µ—Ç–æ–º, –∏—Å–ø–æ–ª—å–∑—É—è LLM –∏ Pydantic –¥–ª—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –≤—ã–≤–æ–¥–∞.
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –æ–±—ä–µ–∫—Ç EvaluationResponse –∏–ª–∏ —Å—Ç—Ä–æ–∫—É —Å –æ—à–∏–±–∫–æ–π.
    """
    evaluator_client = openai.AsyncOpenAI()

    # –ü—Ä–æ–º–ø—Ç –æ–±–Ω–æ–≤–ª–µ–Ω, —á—Ç–æ–±—ã —á–µ—Ç–∫–æ –∏–Ω—Å—Ç—Ä—É–∫—Ç–∏—Ä–æ–≤–∞—Ç—å –º–æ–¥–µ–ª—å —Å–ª–µ–¥–æ–≤–∞—Ç—å Pydantic-—Å—Ö–µ–º–µ.
    system_prompt = """
    You are a strict and precise evaluator. Your sole purpose is to compare a generated agent's answer against a ground-truth "golden" answer based on the original question.

    You must provide your evaluation in a structured JSON format:
    `evaluation`: Your final verdict, which must be either "ok" or "wrong".

    - If the agent's answer is factually correct and fully aligns with the golden answer, set `evaluation` to "ok".
    - If the agent's answer is incorrect, incomplete, or contains any factual errors compared to the golden answer, set `evaluation` to "wrong".
    """

    user_prompt = f"""
    Original Question: "{question}"
    ---
    Golden Answer (Ground Truth): "{golden_answer}"
    ---
    Agent's Answer to Evaluate: "{agent_answer}"
    """

    try:
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ .parse(), –∫–æ—Ç–æ—Ä—ã–π –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –ø–∞—Ä—Å–∏—Ç –æ—Ç–≤–µ—Ç –≤ Pydantic-–º–æ–¥–µ–ª—å
        completion = await evaluator_client.beta.chat.completions.parse(
            model="gpt-4o",
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ],
            response_format=EvaluationResponse,  # –ü–µ—Ä–µ–¥–∞–µ–º –∫–ª–∞—Å—Å Pydantic –Ω–∞–ø—Ä—è–º—É—é
            temperature=0,
        )

        response_message = completion.choices[0].message

        if response_message.parsed:
            # –ï—Å–ª–∏ –ø–∞—Ä—Å–∏–Ω–≥ —É–¥–∞–ª—Å—è, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –≥–æ—Ç–æ–≤—ã–π –æ–±—ä–µ–∫—Ç
            return response_message.parsed
        else:
            # –ï—Å–ª–∏ –º–æ–¥–µ–ª—å –æ—Ç–∫–∞–∑–∞–ª–∞—Å—å –æ—Ç–≤–µ—á–∞—Ç—å –≤ –Ω—É–∂–Ω–æ–º —Ñ–æ—Ä–º–∞—Ç–µ
            print(f"‚ö†Ô∏è Evaluation refusal: {response_message.refusal}")
            return "eval_refusal"

    except Exception as e:
        print(f"‚ùå Error during evaluation: {e}")
        return "eval_error"


# --- –ö–ª–∞—Å—Å ReActAgent---
class ReActAgent:
    def __init__(self, model="gpt-4o-mini", max_iterations=5):
        self.client = openai.AsyncOpenAI()
        self.model = model
        self.max_iterations = max_iterations
        self.user_query = ""
        self.visited_urls = set()
        self.tools = {"search_the_web": self.search_the_web}
        self.tools_specs = [
            {
                "type": "function",
                "function": {
                    "name": "search_the_web",
                    "description": "Searches the internet for information. Can also extract and analyze the full text from web pages.",
                    "parameters": {
                        "type": "object",
                        "properties": {
                            "query": {"type": "string", "description": "A specific, targeted search query."},
                            "max_results": {"type": "integer",
                                            "description": "Maximum number of results to return (default is 3)."}
                        },
                        "required": ["query"],
                    },
                },
            }
        ]

    async def _summarize_content(self, content_to_summarize: str) -> str:
        prompt = SUMMARIZATION_PROMPT.format(user_query=self.user_query, content=content_to_summarize)
        try:
            response = await self.client.chat.completions.create(
                model="gpt-4o-mini", messages=[{"role": "user", "content": prompt}], temperature=0.2,
            )
            return response.choices[0].message.content
        except Exception as e:
            print(f"‚ùå Error during summarization: {e}")
            return "Failed to process text."

    async def search_the_web(self, query: str, max_results: int = 5) -> str:
        """
        –£—Å–æ–≤–µ—Ä—à–µ–Ω—Å—Ç–≤–æ–≤–∞–Ω–Ω—ã–π –ø–æ–∏—Å–∫ —Å –∞–¥–∞–ø—Ç–∏–≤–Ω–æ–π –ª–æ–≥–∏–∫–æ–π:
        1. –ü—ã—Ç–∞–µ—Ç—Å—è –Ω–∞–π—Ç–∏ `max_results` *–Ω–æ–≤—ã—Ö* URL.
        2. –ï—Å–ª–∏ –Ω–∞—Ö–æ–¥–∏—Ç —É–∂–µ –ø–æ—Å–µ—â–µ–Ω–Ω—ã–µ URL, —É–¥–≤–∞–∏–≤–∞–µ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–ø—Ä–∞—à–∏–≤–∞–µ–º—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∏ –¥–µ–ª–∞–µ—Ç –Ω–æ–≤—É—é –ø–æ–ø—ã—Ç–∫—É.
        3. –ï—Å–ª–∏ –≤—Å–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ –ø–µ—Ä–≤–æ–º –∑–∞–ø—Ä–æ—Å–µ –Ω–æ–≤—ã–µ, –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç —Ç–æ–ª—å–∫–æ –ø–µ—Ä–≤—ã–µ max_results.
        """
        print(f"üöÄ Performing smart search for '{query}' with a target of {max_results} new results...")

        final_new_results = []
        # –ú–Ω–æ–∂–µ—Å—Ç–≤–æ –¥–ª—è –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è URL, –∫–æ—Ç–æ—Ä—ã–µ –ø–æ–∏—Å–∫–æ–≤–∏–∫ –≤–µ—Ä–Ω—É–ª –≤ —Ä–∞–º–∫–∞—Ö *—ç—Ç–æ–≥–æ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ* –≤—ã–∑–æ–≤–∞
        # —á—Ç–æ–±—ã –Ω–µ –∑–∞—Ü–∏–∫–ª–∏–≤–∞—Ç—å—Å—è –Ω–∞ –æ–¥–Ω–∏—Ö –∏ —Ç–µ—Ö –∂–µ —Å—Å—ã–ª–∫–∞—Ö
        urls_seen_in_this_search = set()

        current_request_size = max_results
        initial_request = True
        max_attempts = 3  # –ó–∞—â–∏—Ç–∞ –æ—Ç –±–µ—Å–∫–æ–Ω–µ—á–Ω–æ–≥–æ —Ü–∏–∫–ª–∞

        for attempt in range(max_attempts):
            if len(final_new_results) >= max_results:
                break

            print(f"   -> Attempt {attempt + 1}/{max_attempts}: Requesting {current_request_size} results...")

            async with httpx.AsyncClient() as client:
                try:
                    response = await client.post(SEARXNG_ADAPTER_URL, json={
                        "query": query, "max_results": current_request_size, "include_raw_content": True
                    }, timeout=30.0)
                    response.raise_for_status()
                    data = response.json()
                except httpx.RequestError as e:
                    print(f"‚ùå Connection error to SearXNG adapter: {e}")
                    return "Error: Could not connect to the search service."

            if not data.get("results"):
                # –ï—Å–ª–∏ –¥–∞–∂–µ –Ω–∞ —É–≤–µ–ª–∏—á–µ–Ω–Ω—ã–π –∑–∞–ø—Ä–æ—Å –Ω–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤, –≤—ã—Ö–æ–¥–∏–º
                break

            # –§–∏–ª—å—Ç—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã: —Ç–æ–ª—å–∫–æ –Ω–æ–≤—ã–µ –∏ —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ –¥–ª—è —ç—Ç–æ–≥–æ –≤—ã–∑–æ–≤–∞
            new_urls_found_this_attempt = []
            skipped_count = 0
            for res in data["results"]:
                url = res.get('url')
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ URL –µ—Å—Ç—å, –æ–Ω –Ω–µ –±—ã–ª –ø–æ—Å–µ—â–µ–Ω —Ä–∞–Ω–µ–µ –≥–ª–æ–±–∞–ª—å–Ω–æ –∏ –Ω–µ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–ª—Å—è –≤ —ç—Ç–æ–º –ø–æ–∏—Å–∫–µ
                if url and url not in self.visited_urls and url not in urls_seen_in_this_search:
                    new_urls_found_this_attempt.append(res)
                    urls_seen_in_this_search.add(url)
                else:
                    skipped_count += 1

            if new_urls_found_this_attempt:
                final_new_results.extend(new_urls_found_this_attempt)

            # –õ–æ–≥–∏–∫–∞ –ø—Ä–∏–Ω—è—Ç–∏—è —Ä–µ—à–µ–Ω–∏–π
            if initial_request and skipped_count == 0:
                print(
                    f"   -> All {len(final_new_results)} initial results are new. Processing first {max_results} as per rule.")
                final_new_results = final_new_results[:max_results]
                break  # –í—Å–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –Ω–æ–≤—ã–µ, –ø—Ä–µ–∫—Ä–∞—â–∞–µ–º –ø–æ–∏—Å–∫
            elif skipped_count > 0:
                print(f"   -> Found {skipped_count} previously visited URLs. Doubling search size for next attempt.")
                current_request_size *= 2  # –£–¥–≤–∞–∏–≤–∞–µ–º —Ä–∞–∑–º–µ—Ä –¥–ª—è —Å–ª–µ–¥—É—é—â–µ–≥–æ –∑–∞–ø—Ä–æ—Å–∞

            initial_request = False

        if not final_new_results:
            return f"Search for '{query}' yielded no new results to analyze after {max_attempts} attempts."

        # –û–±—Ä–µ–∑–∞–µ–º –¥–æ –Ω—É–∂–Ω–æ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞, –µ—Å–ª–∏ –Ω–∞–±—Ä–∞–ª–∏ –±–æ–ª—å—à–µ
        results_to_process = final_new_results[:max_results]

        # –ü—Ä–æ–¥–æ–ª–∂–∞–µ–º —Å —Å–∞–º–º–∞—Ä–∏–∑–∞—Ü–∏–µ–π, –∫–∞–∫ –∏ —Ä–∞–Ω—å—à–µ
        results_to_summarize = [res for res in results_to_process if res.get("raw_content")]
        if results_to_summarize:
            tasks = [self._summarize_content(res["raw_content"]) for res in results_to_summarize]
            summarized_texts = await asyncio.gather(*tasks)
            for res, summary in zip(results_to_summarize, summarized_texts):
                res["summarized_content"] = summary

        # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º –∏—Ç–æ–≥–æ–≤—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç –∏ –¥–æ–±–∞–≤–ª—è–µ–º –≤ `visited_urls`
        formatted_result = f"Search results for query: '{query}':\n\n"
        for i, result in enumerate(results_to_process, 1):
            url = result['url']
            # –î–æ–±–∞–≤–ª—è–µ–º –≤ –ø–æ—Å–µ—â–µ–Ω–Ω—ã–µ —Ç–æ–ª—å–∫–æ —Ç–µ, –∫–æ—Ç–æ—Ä—ã–µ –º—ã —Ä–µ–∞–ª—å–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–ª–∏
            self.visited_urls.add(url)

            formatted_result += f'[{i}] {result["title"]}\n   URL: {url}\n'
            if "summarized_content" in result:
                formatted_result += f'   Summary: {result["summarized_content"]}\n\n'
            else:
                formatted_result += f'   Snippet: {result.get("content", "N/A")}\n\n'

        return formatted_result

    # *** –ò–ó–ú–ï–ù–ï–ù–ò–ï 1: –ú–µ—Ç–æ–¥ run —Ç–µ–ø–µ—Ä—å –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ª–æ–≤–∞—Ä—å {answer, iterations} ***
    async def run(self, user_query: str) -> dict:
        self.user_query = user_query
        self.visited_urls.clear()
        messages = [{"role": "system", "content": SYSTEM_PROMPT}, {"role": "user", "content": self.user_query}]
        print(f"\nüéØ New task: {self.user_query}")

        for iteration in range(self.max_iterations):
            current_iteration_count = iteration + 1
            print(f"\n=== Iteration {current_iteration_count} ===")
            try:
                response = await self.client.chat.completions.create(
                    model=self.model, messages=messages, tools=self.tools_specs, tool_choice="auto"
                )
                response_message = response.choices[0].message
                if not response_message.tool_calls:
                    final_answer = response_message.content
                    print(f"\n‚úÖ Final Answer (tools no longer needed):\n{final_answer}")
                    return {"answer": final_answer, "iterations": current_iteration_count}

                messages.append(response_message)
                for tool_call in response_message.tool_calls:
                    function_name = tool_call.function.name
                    function_args = json.loads(tool_call.function.arguments)
                    query = function_args.get('query')
                    print(f"ü§î Thought: I need to find information. My query: '{query}'.")
                    function_response = await self.tools[function_name](**function_args)
                    print(
                        f"üßê Observation: I have received processed data (showing snippet):\n---\n{function_response[:1000]}...\n---")
                    messages.append({"tool_call_id": tool_call.id, "role": "tool", "name": function_name,
                                     "content": function_response})

            except Exception as e:
                print(f"An error occurred during iteration {current_iteration_count}: {e}")
                await asyncio.sleep(5)
                return {"answer": f"Error during execution: {e}", "iterations": current_iteration_count}

        print("\nIteration limit reached. Generating final answer...")
        final_response = await self.client.chat.completions.create(model=self.model, messages=messages)
        final_answer = final_response.choices[0].message.content
        return {"answer": final_answer, "iterations": self.max_iterations}


# --- –û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –∑–∞–ø—É—Å–∫–∞ –±–µ–Ω—á–º–∞—Ä–∫–∞ ---
async def main():
    print("1. –ó–∞–≥—Ä—É–∑–∫–∞ –±–µ–Ω—á–º–∞—Ä–∫–∞ 'vtllms/sealqa'...")
    dataset = load_dataset("vtllms/sealqa", name="seal_0", split="test")
    print(f"   ...–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(dataset)} –≤–æ–ø—Ä–æ—Å–æ–≤.")

    NUM_QUESTIONS_TO_RUN = 2
    dataset_subset = dataset.select(range(len(dataset)))

    print(f"\n2. –ó–∞–ø—É—Å–∫ –∞–≥–µ–Ω—Ç–∞ –¥–ª—è {len(dataset_subset)} –≤–æ–ø—Ä–æ—Å–æ–≤...")

    agent = ReActAgent()
    results_data = []

    start_time = time.time()

    for i, item in enumerate(dataset_subset):
        question = item['question']
        golden_answer = item['answer']
        agent_answer = "Not run"
        iterations_count = 0
        evaluation = "Not evaluated"  # –ù–æ–≤–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è –¥–ª—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ –æ—Ü–µ–Ω–∫–∏

        print(f"\n--- [ {i + 1}/{len(dataset_subset)} ] –û–±—Ä–∞–±–æ—Ç–∫–∞ –≤–æ–ø—Ä–æ—Å–∞: {question} ---")

        try:
            agent_result = await agent.run(question)
            agent_answer = agent_result.get("answer", "No answer provided")
            iterations_count = agent_result.get("iterations", 0)

            # *** –ù–û–í–´–ô –®–ê–ì: –í—ã–∑–æ–≤ –æ—Ü–µ–Ω—â–∏–∫–∞ ***
            print(f"‚öñÔ∏è Evaluating the answer...")
            evaluation = await evaluate_answer(question, golden_answer, agent_answer)
        except Exception as e:
            print(f"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–∏ –∞–≥–µ–Ω—Ç–∞ –¥–ª—è –≤–æ–ø—Ä–æ—Å–∞: {question}\n{e}")

        results_data.append({
            '–í–æ–ø—Ä–æ—Å': question,
            '–ì–æ–ª–¥–µ–Ω –æ—Ç–≤–µ—Ç': golden_answer,
            '–û—Ç–≤–µ—Ç –æ—Ç –∞–≥–µ–Ω—Ç–∞': agent_answer,
            '–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∏—Ç–µ—Ä–∞—Ü–∏–π': iterations_count,
            '–û—Ü–µ–Ω–∫–∞': evaluation  # *** –ù–û–í–´–ô –®–ê–ì: –î–æ–±–∞–≤–ª—è–µ–º –æ—Ü–µ–Ω–∫—É –≤ –¥–∞–Ω–Ω—ã–µ ***
        })
        print(f"--- [ {i + 1}/{len(dataset_subset)} ] –í–æ–ø—Ä–æ—Å –æ–±—Ä–∞–±–æ—Ç–∞–Ω –∑–∞ {iterations_count} –∏—Ç–µ—Ä–∞—Ü–∏–π ---")

    end_time = time.time()
    total_time = end_time - start_time
    print(f"\n3. –û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞ –∑–∞ {total_time:.2f} —Å–µ–∫—É–Ω–¥.")

    print("\n4. –°–æ–∑–¥–∞–Ω–∏–µ Excel –æ—Ç—á–µ—Ç–∞...")
    df = pd.DataFrame(results_data)
    # *** –ù–û–í–´–ô –®–ê–ì: –î–æ–±–∞–≤–ª—è–µ–º –Ω–æ–≤—É—é –∫–æ–ª–æ–Ω–∫—É –≤ –æ—Ç—á–µ—Ç ***
    df = df[['–í–æ–ø—Ä–æ—Å', '–ì–æ–ª–¥–µ–Ω –æ—Ç–≤–µ—Ç', '–û—Ç–≤–µ—Ç –æ—Ç –∞–≥–µ–Ω—Ç–∞', '–û—Ü–µ–Ω–∫–∞', '–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∏—Ç–µ—Ä–∞—Ü–∏–π']]
    output_filename = "benchmark_results_with_evaluation.xlsx"
    df.to_excel(output_filename, index=False, engine='openpyxl')

    print(f"   ...–û—Ç—á–µ—Ç —É—Å–ø–µ—à–Ω–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ —Ñ–∞–π–ª: {output_filename}")


if __name__ == "__main__":
    asyncio.run(main())
